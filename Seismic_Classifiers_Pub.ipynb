{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "Seismic Classifiers-Pub.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aiden213/Deep-Learning/blob/master/Seismic_Classifiers_Pub.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNowZZN1uvEv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        },
        "outputId": "5293f562-10b1-4605-ff90-dd5bea645f76"
      },
      "source": [
        "!pip install tensorflow-gpu==2.3.0"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==2.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/11/763f55d3d15efd778ef24453f126e6c33635680e5a2bb346da3fab5997cb/tensorflow_gpu-2.3.0-cp36-cp36m-manylinux2010_x86_64.whl (320.4MB)\n",
            "\u001b[K     |████████████████████████████████| 320.4MB 50kB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.3.0) (3.3.0)\n",
            "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.3.0) (1.18.5)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.3.0) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.3.0) (1.12.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.3.0) (0.9.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.3.0) (0.2.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.3.0) (3.12.4)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.3.0) (2.10.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.3.0) (1.6.3)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.3.0) (1.4.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.3.0) (1.30.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.3.0) (2.3.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.3.0) (0.3.3)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.3.0) (1.1.2)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.3.0) (2.3.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.3.0) (1.15.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.3.0) (0.34.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.9.2->tensorflow-gpu==2.3.0) (49.2.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.0) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.0) (0.4.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.0) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.0) (1.7.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.0) (1.17.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.0) (3.2.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.0) (1.3.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.0) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.0) (2.10)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.0) (4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.0) (4.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.0) (0.2.8)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.0) (1.7.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.0) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.0) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.0) (3.1.0)\n",
            "Installing collected packages: tensorflow-gpu\n",
            "Successfully installed tensorflow-gpu-2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85n2dBbLwWO5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6ef7b041-7aa3-468e-8a87-5698a6c33361"
      },
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        " raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50qDINfywpFE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "outputId": "aef43aad-3436-45bc-8257-ddaa07115e4e"
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[name: \"/device:CPU:0\"\n",
              " device_type: \"CPU\"\n",
              " memory_limit: 268435456\n",
              " locality {\n",
              " }\n",
              " incarnation: 6716042898854728006, name: \"/device:XLA_CPU:0\"\n",
              " device_type: \"XLA_CPU\"\n",
              " memory_limit: 17179869184\n",
              " locality {\n",
              " }\n",
              " incarnation: 9927295753382729350\n",
              " physical_device_desc: \"device: XLA_CPU device\", name: \"/device:XLA_GPU:0\"\n",
              " device_type: \"XLA_GPU\"\n",
              " memory_limit: 17179869184\n",
              " locality {\n",
              " }\n",
              " incarnation: 4613394800182111354\n",
              " physical_device_desc: \"device: XLA_GPU device\", name: \"/device:GPU:0\"\n",
              " device_type: \"GPU\"\n",
              " memory_limit: 11133970048\n",
              " locality {\n",
              "   bus_id: 1\n",
              "   links {\n",
              "   }\n",
              " }\n",
              " incarnation: 1200412819387810251\n",
              " physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmayNR3SxCu7",
        "colab_type": "text"
      },
      "source": [
        "for cpu and ram info:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEszmO2txFHh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "793d4363-e912-4c39-cff2-d61dd68ab97d"
      },
      "source": [
        "!cat /proc/cpuinfo\n",
        "!cat /proc/meminfo"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "processor\t: 0\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 63\n",
            "model name\t: Intel(R) Xeon(R) CPU @ 2.30GHz\n",
            "stepping\t: 0\n",
            "microcode\t: 0x1\n",
            "cpu MHz\t\t: 2300.000\n",
            "cache size\t: 46080 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 2\n",
            "core id\t\t: 0\n",
            "cpu cores\t: 1\n",
            "apicid\t\t: 0\n",
            "initial apicid\t: 0\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 13\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid xsaveopt arat md_clear arch_capabilities\n",
            "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs\n",
            "bogomips\t: 4600.00\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n",
            "processor\t: 1\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 63\n",
            "model name\t: Intel(R) Xeon(R) CPU @ 2.30GHz\n",
            "stepping\t: 0\n",
            "microcode\t: 0x1\n",
            "cpu MHz\t\t: 2300.000\n",
            "cache size\t: 46080 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 2\n",
            "core id\t\t: 0\n",
            "cpu cores\t: 1\n",
            "apicid\t\t: 1\n",
            "initial apicid\t: 1\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 13\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid xsaveopt arat md_clear arch_capabilities\n",
            "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs\n",
            "bogomips\t: 4600.00\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n",
            "MemTotal:       13333556 kB\n",
            "MemFree:         6166540 kB\n",
            "MemAvailable:   12015044 kB\n",
            "Buffers:          108988 kB\n",
            "Cached:          5651744 kB\n",
            "SwapCached:            0 kB\n",
            "Active:          1124036 kB\n",
            "Inactive:        5537332 kB\n",
            "Active(anon):     760720 kB\n",
            "Inactive(anon):     2388 kB\n",
            "Active(file):     363316 kB\n",
            "Inactive(file):  5534944 kB\n",
            "Unevictable:           0 kB\n",
            "Mlocked:               0 kB\n",
            "SwapTotal:             0 kB\n",
            "SwapFree:              0 kB\n",
            "Dirty:               704 kB\n",
            "Writeback:             0 kB\n",
            "AnonPages:        900736 kB\n",
            "Mapped:           505136 kB\n",
            "Shmem:              3012 kB\n",
            "Slab:             321292 kB\n",
            "SReclaimable:     275528 kB\n",
            "SUnreclaim:        45764 kB\n",
            "KernelStack:        3648 kB\n",
            "PageTables:         7564 kB\n",
            "NFS_Unstable:          0 kB\n",
            "Bounce:                0 kB\n",
            "WritebackTmp:          0 kB\n",
            "CommitLimit:     6666776 kB\n",
            "Committed_AS:    3044864 kB\n",
            "VmallocTotal:   34359738367 kB\n",
            "VmallocUsed:           0 kB\n",
            "VmallocChunk:          0 kB\n",
            "Percpu:              936 kB\n",
            "AnonHugePages:         0 kB\n",
            "ShmemHugePages:        0 kB\n",
            "ShmemPmdMapped:        0 kB\n",
            "HugePages_Total:       0\n",
            "HugePages_Free:        0\n",
            "HugePages_Rsvd:        0\n",
            "HugePages_Surp:        0\n",
            "Hugepagesize:       2048 kB\n",
            "Hugetlb:               0 kB\n",
            "DirectMap4k:      152764 kB\n",
            "DirectMap2M:     7186432 kB\n",
            "DirectMap1G:     8388608 kB\n",
            "cat: /proc/gpuinfo: No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4vlzhTruMfn",
        "colab_type": "text"
      },
      "source": [
        "First we'll import all the libraries we need down the line. We also set the \"random seed\", so results can be reproduced by avid readers. Keras should report using the Tensorflow backend, otherwise reproducibility cannot be guaranteed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZGLl1HXxp1p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "outputId": "f02d8d70-c486-4f7b-950e-4d762d40165c"
      },
      "source": [
        "!pip install obspy"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting obspy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b8/8c/eef47074a1884c73bc4f2ba7b2961a79fc54952edadeff4b998de86dcb20/obspy-1.2.2.zip (24.7MB)\n",
            "\u001b[K     |████████████████████████████████| 24.7MB 129kB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from obspy) (49.2.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from obspy) (4.4.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from obspy) (2.23.0)\n",
            "Requirement already satisfied: sqlalchemy in /usr/local/lib/python3.6/dist-packages (from obspy) (1.3.18)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (from obspy) (4.2.6)\n",
            "Requirement already satisfied: matplotlib>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from obspy) (3.2.2)\n",
            "Requirement already satisfied: scipy>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from obspy) (1.4.1)\n",
            "Requirement already satisfied: future>=0.12.4 in /usr/local/lib/python3.6/dist-packages (from obspy) (0.16.0)\n",
            "Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from obspy) (1.18.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->obspy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->obspy) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->obspy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->obspy) (2.10)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.1.0->obspy) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.1.0->obspy) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.1.0->obspy) (1.2.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.1.0->obspy) (2.4.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib>=1.1.0->obspy) (1.15.0)\n",
            "Building wheels for collected packages: obspy\n",
            "  Building wheel for obspy (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for obspy: filename=obspy-1.2.2-cp36-cp36m-linux_x86_64.whl size=21666241 sha256=70f612e4b67fcb503e51c36535bd36b1328ff8c7f2ed6957209331b248519afb\n",
            "  Stored in directory: /root/.cache/pip/wheels/94/e0/86/44fd4cea7661f42431c8f6d030f2758ff275cccffcbe8fa2b8\n",
            "Successfully built obspy\n",
            "Installing collected packages: obspy\n",
            "Successfully installed obspy-1.2.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VT7IU0PhxiIw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import obspy        #additional module\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tqdm\n",
        "from tqdm import tnrange, tqdm_notebook\n",
        "\n",
        "from obspy.io.segy.segy import _read_segy\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "np.random.seed(42)\n",
        "%matplotlib notebook\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GoZAQNF5uMft",
        "colab_type": "text"
      },
      "source": [
        "For experimentation with network models, we keep the keras imports separate, to reduce loading time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8_8gZSr1O6I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Sequential, Model, clone_model\n",
        "from tensorflow.keras.layers import Conv2D, Dense, Activation, Flatten, Dropout, Input, BatchNormalization\n",
        "\n",
        "#from tensorflow.keras.layers.normalization import BatchNormalization  tf2.3\"BatchNormalization\"直接在layers下了\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import SGD\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b92psKHuMfx",
        "colab_type": "text"
      },
      "source": [
        "We need to define some parameters. As we are using Transfer learning, we have to adjust these parameters to fit into the network that we use and test. \n",
        "\n",
        "| Model   |      Channels      |  Patch-Size |\n",
        "|----------|:-------------:|------:|\n",
        "| Waldeland |  1 | 64 |\n",
        "| VGG16 |    3   |  64 |\n",
        "| ResNet50 | 3 | 244 |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4-5Dt6TuMfy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "patch_size = 64 # for ResNet50 put 244\n",
        "batch_size = 256\n",
        "num_channels = 1\n",
        "num_classes = 9\n",
        "all_examples = 158812\n",
        "num_examples = 7500\n",
        "epochs = 20\n",
        "steps=450\n",
        "sampler = list(range(all_examples))\n",
        "\n",
        "opt = 'adam'\n",
        "lossfkt = ['categorical_crossentropy']\n",
        "metrica = ['mae', 'acc']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyIoDGoWuMf2",
        "colab_type": "text"
      },
      "source": [
        "Here we test, whether we are running on CPU or GPU. We want to run on GPU, if it's not in the device list. It will be slow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFKI43KUuMf3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())\n",
        "# It should say GPU here. Otherwise your model will run sloooow."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNW8EcJzuMf7",
        "colab_type": "text"
      },
      "source": [
        "# Data Loading\n",
        "Now let's load the F3 data and read three slices. The labeled data, as well as, a distal inline and a crossline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPRamdjcuMf8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filename = 'data/Dutch Government_F3_entire_8bit seismic.segy'\n",
        "\n",
        "t0=time.time()\n",
        "stream0 = _read_segy(filename, headonly=True)\n",
        "print('--> data read in {:.1f} sec'.format(time.time()-t0)) #Thanks to aadm \n",
        "\n",
        "t0=time.time()\n",
        "\n",
        "labeled_data = np.stack(t.data for t in stream0.traces if t.header.for_3d_poststack_data_this_field_is_for_in_line_number == 339).T\n",
        "inline_data = np.stack(t.data for t in stream0.traces if t.header.for_3d_poststack_data_this_field_is_for_in_line_number == 500).T\n",
        "xline_data = np.stack(t.data for t in stream0.traces if t.header.for_3d_poststack_data_this_field_is_for_cross_line_number == 500).T\n",
        "\n",
        "print('--> created slices in {:.1f} sec'.format(time.time()-t0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NT4f4cuHuMf_",
        "colab_type": "text"
      },
      "source": [
        "# Helper Functions\n",
        "From these slices, we need to extract patches. While, we could do that before and save them as array or image data, using a generator that utilizes the CPU, while the GPU trains the network is a bit more storage- and memory-friendly. `patch_extractor2D()` automates the patch-extraction and pads sides, where necessary.\n",
        "\n",
        "Then we build `acc_assess()` to format our test accuracy assessment nicely, because we're lazy and retyping it for every model we build is a nuisance.\n",
        "\n",
        "All functions are accompanied with a little sanity check. While this is not automated testing (like TDD), it does help to make sure, our function works as intended."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjE3h902uMgA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def patch_extractor2D(img,mid_x,mid_y,patch_size,dimensions=1):\n",
        "    try:\n",
        "        x,y,c = img.shape\n",
        "    except ValueError:\n",
        "        x,y = img.shape\n",
        "        c=1\n",
        "    patch= np.pad(img, patch_size//2, 'constant', constant_values=0)[mid_y:mid_y+patch_size,mid_x:mid_x+patch_size] #because it's padded we don't subtract half patches all the tim\n",
        "    if c != dimensions:\n",
        "        tmp_patch = np.zeros((patch_size,patch_size,dimensions))\n",
        "        for uia in range(dimensions):\n",
        "            tmp_patch[:,:,uia] = patch\n",
        "        return tmp_patch\n",
        "    return patch\n",
        "image=np.random.rand(10,10)//.1\n",
        "print(image)\n",
        "\n",
        "patch_extractor2D(image,10,10,4,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lH9DOIt6uMgE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def acc_assess(data,loss=['categorical_crossentropy'],metrics=['acc']):\n",
        "    if not isinstance(loss, list):\n",
        "        try:\n",
        "            loss = [loss]\n",
        "        except:\n",
        "            raise(\"Loss must be list.\")\n",
        "    if not isinstance(metrics, list):\n",
        "        try:\n",
        "            metrics = [metrics]\n",
        "        except:\n",
        "            raise(\"Metrics must be list.\")\n",
        "    out='The test loss is {:.3f}\\n'.format(data[0])\n",
        "    for i, metric in enumerate(metrics):            \n",
        "        if metric in 'mae':\n",
        "            out += \"The total mean error on the test is {:.3f}\\n\".format(data[i+1])\n",
        "        if metric in 'accuracy':\n",
        "            out += \"The test accuracy is {:.1f}%\\n\".format(data[i+1]*100)\n",
        "    return out\n",
        "print(acc_assess([1,2,3],'bla',[\"acc\", \"mae\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QMNyWgeuMgH",
        "colab_type": "text"
      },
      "source": [
        "# Exploratory Data Analysis\n",
        "We need to load and check our labels. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGL-RinSuMgH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = pd.read_csv('data/classification.ixz', delimiter=\" \", names=[\"Inline\",\"Xline\",\"Time\",\"Class\"])\n",
        "labels.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "rOTe-DnCuMgK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels[\"Xline\"]-=300-1\n",
        "labels[\"Time\"] = labels[\"Time\"]//4\n",
        "labels.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mIFU80BuMgN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labeled_data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "ni-h8UkguMgQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig2 = plt.figure(figsize=(15.0, 10.0))\n",
        "vml = np.percentile(labeled_data, 99)\n",
        "img1 = plt.imshow(labeled_data, cmap=\"Greys\", vmin=-vml, vmax=vml, aspect='auto')\n",
        "plt.yticks(np.arange(0, 462, 100), np.arange(0, 462*4, 400))\n",
        "plt.xlabel('Trace Location')\n",
        "plt.ylabel('Time [ms]')\n",
        "plt.savefig('labeled_data.png', bbox_inches='tight')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGGAc7SEuMgT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig2 = plt.figure(figsize=(15.0, 10.0))\n",
        "vmx = np.percentile(xline_data, 99)\n",
        "plt.imshow(xline_data, cmap=\"Greys\", vmin=-vmx, vmax=vmx, aspect='auto')\n",
        "plt.yticks(np.arange(0, 462, 100), np.arange(0, 462*4, 400))\n",
        "plt.xlabel('Trace Location')\n",
        "plt.ylabel('Time [ms]')\n",
        "plt.savefig('xline_data.png', bbox_inches='tight')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "29s5XQR8uMgX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig2 = plt.figure(figsize=(15.0, 10.0))\n",
        "vmy = np.percentile(inline_data, 99)\n",
        "plt.imshow(inline_data, cmap=\"Greys\", vmin=-vmy, vmax=vmy, aspect='auto')\n",
        "plt.yticks(np.arange(0, 462, 100), np.arange(0, 462*4, 400))\n",
        "plt.xlabel('Trace Location')\n",
        "plt.ylabel('Time [ms]')\n",
        "plt.savefig('inline_data.png', bbox_inches='tight')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEZjN_SSuMgZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig2 = plt.figure(figsize=(15.0, 10.0))\n",
        "img2 = plt.imshow(labeled_data, cmap=\"Greys\", vmin=-vml, vmax=vml, aspect='auto')\n",
        "img1 = plt.scatter(labels[\"Xline\"],labels[[\"Time\"]],c=labels[[\"Class\"]],cmap='Dark2',alpha=0.03)\n",
        "plt.yticks(np.arange(0, 462, 100), np.arange(0, 462*4, 400))\n",
        "plt.xlabel('Trace Location')\n",
        "plt.ylabel('Time [ms]')\n",
        "plt.savefig('label.png', bbox_inches='tight')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v--cHw8WuMgc",
        "colab_type": "text"
      },
      "source": [
        "# Train the Network\n",
        "Now we perform a test-train split. Then we can validate the results of our experiment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKk11WVIuMgd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data, test_data, train_samples, test_samples = train_test_split(\n",
        "    labels, sampler, random_state=42)\n",
        "print(train_data.shape,test_data.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3mSFav1uMgg",
        "colab_type": "text"
      },
      "source": [
        "This is the `keras` data generator that wraps the `patch_extractor2D()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmxDyb68uMgh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SeismicSequence(keras.utils.Sequence):\n",
        "    def __init__(self, img, x_set, t_set, y_set, patch_size, batch_size, dimensions):\n",
        "        self.slice = img\n",
        "        self.X,self.t = x_set,t_set\n",
        "        self.batch_size = batch_size\n",
        "        self.patch_size = patch_size\n",
        "        self.dimensions = dimensions\n",
        "        self.label = y_set\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.X) // self.batch_size\n",
        "    \n",
        "    def __getitem__(self,idx):\n",
        "        sampler = np.random.permutation(len(self.X))\n",
        "        samples = sampler[idx*self.batch_size:(idx+1)*self.batch_size]\n",
        "        labels = keras.utils.to_categorical(self.label[samples], num_classes=9)\n",
        "        if self.dimensions == 1:\n",
        "            return np.expand_dims(np.array([patch_extractor2D(self.slice,self.X[x],self.t[x],self.patch_size,self.dimensions) for x in samples]), axis=4), labels\n",
        "        else:\n",
        "            return np.array([patch_extractor2D(self.slice,self.X[x],self.t[x],self.patch_size,self.dimensions) for x in samples]), labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G95DhUXWuMgk",
        "colab_type": "text"
      },
      "source": [
        "We define several callbacks for keras. The training should be stopped early, if the validation loss or the categorical cross entropy do not improve within the defined patience. Checkpoints are written to `tmp.h5` for every epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOtET5XJuMgl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "earlystop1 = keras.callbacks.EarlyStopping(monitor='val_loss',\n",
        "                              min_delta=0,\n",
        "                              patience=3,\n",
        "                              verbose=0, mode='auto')\n",
        "\n",
        "earlystop2 = keras.callbacks.EarlyStopping(monitor='val_acc',\n",
        "                              min_delta=0,\n",
        "                              patience=3,\n",
        "                              verbose=0, mode='auto')\n",
        "\n",
        "checkpoint = keras.callbacks.ModelCheckpoint('tmp.h5', \n",
        "                                     monitor='val_loss', \n",
        "                                     verbose=0, \n",
        "                                     save_best_only=False, \n",
        "                                     save_weights_only=False, \n",
        "                                     mode='auto', \n",
        "                                     period=1)\n",
        "\n",
        "callbacklist = [TQDMNotebookCallback(leave_inner=True, leave_outer=True), earlystop1, earlystop2, checkpoint]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OangE02uMgp",
        "colab_type": "text"
      },
      "source": [
        "## Waldeland CNN\n",
        "The model introduced by Waldeland, reproduced from MalenoV. Compared to today's standards this is a relatively shallow CNN. We train the network from scratch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzD7YFnguMgp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "\n",
        "model_vanilla = Sequential()\n",
        "model_vanilla.add(Conv2D(50, (5, 5), padding='same', input_shape=(patch_size,patch_size,1), strides=(4, 4), data_format=\"channels_last\",name = 'conv_layer1'))\n",
        "model_vanilla.add(BatchNormalization())\n",
        "model_vanilla.add(Activation('relu'))\n",
        "model_vanilla.add(Conv2D(50, (3, 3), strides=(2, 2), padding = 'same',name = 'conv_layer2'))\n",
        "model_vanilla.add(Dropout(0.5))\n",
        "model_vanilla.add(BatchNormalization())\n",
        "model_vanilla.add(Activation('relu'))\n",
        "model_vanilla.add(Conv2D(50, (3, 3), strides=(2, 2), padding= 'same',name = 'conv_layer3'))\n",
        "model_vanilla.add(Dropout(0.4))\n",
        "model_vanilla.add(BatchNormalization())\n",
        "model_vanilla.add(Activation('relu'))\n",
        "model_vanilla.add(Conv2D(50, (3, 3), strides=(2, 2), padding= 'same',name = 'conv_layer4'))\n",
        "model_vanilla.add(Dropout(0.2))\n",
        "model_vanilla.add(BatchNormalization())\n",
        "model_vanilla.add(Activation('relu'))\n",
        "model_vanilla.add(Conv2D(50, (3, 3), strides=(2, 2), padding= 'same',name = 'conv_layer5'))\n",
        "model_vanilla.add(Flatten())\n",
        "model_vanilla.add(Dense(50,name = 'dense_layer1'))\n",
        "model_vanilla.add(BatchNormalization())\n",
        "model_vanilla.add(Activation('relu'))\n",
        "model_vanilla.add(Dense(10,name = 'attribute_layer'))\n",
        "model_vanilla.add(BatchNormalization())\n",
        "model_vanilla.add(Activation('relu'))\n",
        "model_vanilla.add(Dense(num_classes, name = 'pre-softmax_layer'))\n",
        "model_vanilla.add(BatchNormalization())\n",
        "model_vanilla.add(Activation('softmax'))\n",
        "\n",
        "model_vanilla.compile(loss=lossfkt,\n",
        "                  optimizer=opt,\n",
        "                  metrics=metrica)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "-49LULH1uMgs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t0=time.time()\n",
        "\n",
        "hist_vanilla = model_vanilla.fit_generator(\n",
        "    SeismicSequence(\n",
        "        labeled_data,\n",
        "        train_data[\"Xline\"].values,\n",
        "        train_data[\"Time\"].values,\n",
        "        train_data[\"Class\"].values,\n",
        "        patch_size,\n",
        "        batch_size,\n",
        "        1),\n",
        "    steps_per_epoch=steps,\n",
        "    validation_data = SeismicSequence(\n",
        "        labeled_data,\n",
        "        test_data[\"Xline\"].values,\n",
        "        test_data[\"Time\"].values,\n",
        "        test_data[\"Class\"].values,\n",
        "        patch_size,\n",
        "        batch_size,\n",
        "        1),\n",
        "    validation_steps = len(test_samples)//batch_size,\n",
        "    epochs = epochs,\n",
        "    verbose = 0,\n",
        "    callbacks = callbacklist)\n",
        "\n",
        "print('--> Training for Waldeland CNN took {:.1f} sec'.format(time.time()-t0)) #Thanks to aadm "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLIqNcThuMgv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_vanilla.save(\"vanilla_model.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "2U-axUIouMgx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vanillascore=model_vanilla.evaluate(np.expand_dims(np.array([patch_extractor2D(labeled_data,labels[\"Xline\"][x],labels[\"Time\"][x],64) for x in test_samples]), axis=4),keras.utils.to_categorical(labels[\"Class\"][test_samples], num_classes=9), verbose=0)\n",
        "print(acc_assess(vanillascore,lossfkt,metrica))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlVC4oIjuMg0",
        "colab_type": "text"
      },
      "source": [
        "Looking at the metric on training as well as validation gives a good overview, if we are doing appropriate training or if we are overfitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "3a5Qwx3MuMg0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(hist_vanilla.history.keys())\n",
        "plt.plot(hist_vanilla.history['acc'])\n",
        "plt.plot(hist_vanilla.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "hI87H5wYuMg4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# summarize history for loss\n",
        "plt.plot(hist_vanilla.history['loss'])\n",
        "plt.plot(hist_vanilla.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "rC0cu7AvuMg7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t_max, y_max = xline_data.shape\n",
        "\n",
        "half_patch = patch_size//2\n",
        "\n",
        "predx = np.full_like(xline_data,-1)\n",
        "\n",
        "for space in tqdm_notebook(range(y_max),desc='Space'):\n",
        "    for depth in tqdm_notebook(range(t_max),leave=False, desc='Time'):\n",
        "        predx[depth,space] = np.argmax(model_vanilla.predict(np.expand_dims(np.expand_dims(patch_extractor2D(xline_data,space,depth,patch_size), axis=0), axis=4)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4Fya-_JuMg_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.save('vanilla_predx.npy',predx,allow_pickle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1gSzsY_uMhB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.imshow(predx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "uagKzFZiuMhD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig2 = plt.figure(figsize=(15.0, 10.0))\n",
        "img2 = plt.imshow(xline_data, cmap=\"Greys\", vmin=-vmx, vmax=vmx, aspect='auto')\n",
        "img1 = plt.imshow(predx, aspect='auto', cmap=\"Dark2\", alpha=0.5)\n",
        "plt.savefig('pred1_x.png', bbox_inches='tight')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "vLhsxcvXuMhG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t_max, y_max = inline_data.shape\n",
        "\n",
        "half_patch = patch_size//2\n",
        "\n",
        "predi= np.full_like(inline_data,-1)\n",
        "\n",
        "for space in tqdm_notebook(range(y_max),desc='Space'):\n",
        "    for depth in tqdm_notebook(range(t_max),leave=False, desc='Time'):\n",
        "        predi[depth,space] = np.argmax(model_vanilla.predict(np.expand_dims(np.expand_dims(patch_extractor2D(inline_data,space,depth,patch_size), axis=0), axis=4)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79j8cS4puMhI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.save('vanilla_predi.npy',predi,allow_pickle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tC0WfpHFuMhM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predi = np.load('vanilla_predi.npy')\n",
        "plt.imshow(predi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "p2nqJVc_uMhO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.save('vanilla_predi.npy',predi,allow_pickle=False)\n",
        "fig2 = plt.figure(figsize=(15.0, 10.0))\n",
        "img2 = plt.imshow(inline_data, cmap=\"Greys\", vmin=-vmy, vmax=vmy, aspect='auto')\n",
        "img1 = plt.imshow(predi, aspect='auto', cmap=\"Dark2\", alpha=0.5)\n",
        "plt.yticks(np.arange(0, 462, 100), np.arange(0, 462*4, 400))\n",
        "plt.xlabel('Trace Location')\n",
        "plt.ylabel('Time [ms]')\n",
        "plt.savefig('pred1_i.png', bbox_inches='tight')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlpEnu2WuMhR",
        "colab_type": "text"
      },
      "source": [
        "## VGG16 Transfer Learning\n",
        "We import the VGG16 model trained on the ImageNet dataset. We freeze all layers and cut off the classification part. We can then retrain the classification neurons, to see if the filters generalize to seismic data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3r-NqhGuMhS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.applications.vgg16 import VGG16\n",
        "from keras import backend as K\n",
        "K.set_image_dim_ordering('tf')   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COUYrjGTuMhV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_tensor = Input(shape=(patch_size,patch_size,3))\n",
        "base_model = keras.applications.vgg16.VGG16(include_top=False, weights='imagenet', input_tensor=input_tensor, input_shape=None)\n",
        "\n",
        "for layer in base_model.layers[:8]:\n",
        "    layer.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTlseM1zuMhY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = base_model.output\n",
        "x = Flatten()(x)\n",
        "x = Dense(256,name = 'dense_layer1')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation('relu')(x)\n",
        "x = Dropout(.5)(x)\n",
        "x = Dense(num_classes, name = 'pre-softmax_layer')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation('softmax')(x)\n",
        "\n",
        "vgg = Model(input=base_model.input, output=x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7zoShgzuMhb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sgd = SGD(lr=1e-4, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "vgg.compile(loss=lossfkt,\n",
        "                  optimizer=sgd,\n",
        "                  metrics=metrica)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "UE5XtNmjuMhe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t0 = time.time()\n",
        "\n",
        "vgg_hist = vgg.fit_generator(\n",
        "    SeismicSequence(\n",
        "        labeled_data,\n",
        "        train_data[\"Xline\"].values,\n",
        "        train_data[\"Time\"].values,\n",
        "        train_data[\"Class\"].values,\n",
        "        patch_size,\n",
        "        batch_size,\n",
        "        3),\n",
        "    steps_per_epoch=steps,\n",
        "    validation_data = SeismicSequence(\n",
        "        labeled_data,\n",
        "        test_data[\"Xline\"].values,\n",
        "        test_data[\"Time\"].values,\n",
        "        test_data[\"Class\"].values,\n",
        "        patch_size,\n",
        "        batch_size,\n",
        "        3),\n",
        "    validation_steps = len(test_data)//batch_size,\n",
        "    epochs = epochs,\n",
        "    verbose = 0,\n",
        "    callbacks = callbacklist)\n",
        "\n",
        "print('--> Training for VGG transfer took {:.1f} sec'.format(time.time()-t0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9uaEbQxuMhg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vgg.save('vgg_model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "jlQMrKFhuMhk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vggscore=vgg.evaluate(np.array([patch_extractor2D(labeled_data,labels[\"Xline\"][x],labels[\"Time\"][x],64,3) for x in test_samples]), keras.utils.to_categorical(labels[\"Class\"][test_samples], num_classes=9))\n",
        "print(acc_assess(vggscore,lossfkt,metrica))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEyAMn5VuMhm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(hist_vanilla.history.keys())\n",
        "plt.plot(vgg_hist.history['acc'])\n",
        "plt.plot(vgg_hist.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mc7bDvxVuMhq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# summarize history for loss\n",
        "plt.plot(vgg_hist.history['loss'])\n",
        "plt.plot(vgg_hist.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "-clabtGluMhs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t_max, y_max = xline_data.shape\n",
        "\n",
        "half_patch = patch_size//2\n",
        "\n",
        "vgg_predx = np.full_like(xline_data,-1)\n",
        "\n",
        "for space in tqdm_notebook(range(y_max),desc='Space'):\n",
        "    for depth in tqdm_notebook(range(t_max),leave=False, desc='Time'):\n",
        "        vgg_predx[depth,space] = np.argmax(vgg.predict(np.expand_dims(patch_extractor2D(xline_data,space,depth,patch_size,3), axis=0)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPwHrhcZuMhx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.save('vgg_predx.npy',vgg_predx,allow_pickle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ok-fay9BuMhz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.imshow(vgg_predx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHTR2CeUuMh2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vgg_predx=np.load('vgg_predx.npy')\n",
        "fig2 = plt.figure(figsize=(15.0, 10.0))\n",
        "img2 = plt.imshow(xline_data, cmap=\"Greys\", vmin=-vmx, vmax=vmx, aspect='auto')\n",
        "img1 = plt.imshow(vgg_predx, aspect='auto', cmap=\"Dark2\", alpha=0.5)\n",
        "plt.yticks(np.arange(0, 462, 100), np.arange(0, 462*4, 400))\n",
        "plt.xlabel('Trace Location')\n",
        "plt.ylabel('Time [ms]')\n",
        "plt.savefig('vgg1_x.png', bbox_inches='tight')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "-83kLnj_uMh4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t_max, y_max = inline_data.shape\n",
        "\n",
        "half_patch = patch_size//2\n",
        "\n",
        "vgg_predi = np.full_like(inline_data,-1)\n",
        "\n",
        "for space in tqdm_notebook(range(y_max),desc='Space'):\n",
        "    for depth in tqdm_notebook(range(t_max),leave=False, desc='Time'):\n",
        "        vgg_predi[depth,space] = np.argmax(vgg.predict(np.expand_dims(patch_extractor2D(inline_data,space,depth,patch_size,3), axis=0)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOuAE-TluMh7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.save('vgg_predi.npy',vgg_predi,allow_pickle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJqsqf4AuMh9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.imshow(vgg_predi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqbHFQyJuMh_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vgg_predi= np.load('vgg_predi.npy')\n",
        "fig2 = plt.figure(figsize=(15.0, 10.0))\n",
        "img2 = plt.imshow(inline_data, cmap=\"Greys\", vmin=-vmy, vmax=vmy, aspect='auto')\n",
        "img1 = plt.imshow(vgg_predi, aspect='auto', cmap=\"Dark2\", alpha=0.5)\n",
        "plt.yticks(np.arange(0, 462, 100), np.arange(0, 462*4, 400))\n",
        "plt.xlabel('Trace Location')\n",
        "plt.ylabel('Time [ms]')\n",
        "plt.savefig('vgg1_i.png', bbox_inches='tight')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfhLqxrEuMiC",
        "colab_type": "text"
      },
      "source": [
        "## ResNet50 Transfer Learning\n",
        "We import the ResNet50 that was trained on the ImageNet data and freeze all layers, like we did for the VGG16. Then we retrain the classifier to see if the learned filters generalize on seismic data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtthkjMSuMiC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.applications.resnet50 import ResNet50\n",
        "from keras import backend as K\n",
        "K.set_image_dim_ordering('tf')   \n",
        "patch_size=244"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-c8y9u-PuMiF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_tensor = Input(shape=(patch_size,patch_size,3))\n",
        "res_base = ResNet50(include_top=False, weights='imagenet', input_tensor=input_tensor, input_shape=None, pooling=None)\n",
        "\n",
        "for layer in res_base.layers[:45]:\n",
        "    layer.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzYuG3r9uMiM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "q = res_base.output\n",
        "q = Flatten()(q)\n",
        "q = BatchNormalization()(q)\n",
        "q = Activation('relu')(q)\n",
        "q = Dense(10,name = 'attribute_layer')(q)\n",
        "q = BatchNormalization()(q)\n",
        "q = Activation('relu')(q)\n",
        "q = Dense(num_classes, name = 'pre-softmax_layer')(q)\n",
        "q = BatchNormalization()(q)\n",
        "q = Activation('softmax')(q)\n",
        "resnet = Model(input=res_base.input, output=q)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7ZakaGGuMiP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "resnet.compile(loss=lossfkt,\n",
        "                  optimizer=opt,\n",
        "                  metrics=metrica)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_dj7Ra3uMiS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t0 = time.time()\n",
        "\n",
        "batch_size=50\n",
        "res_hist = resnet.fit_generator(\n",
        "    SeismicSequence(\n",
        "        labeled_data,\n",
        "        train_data[\"Xline\"].values,\n",
        "        train_data[\"Time\"].values,\n",
        "        train_data[\"Class\"].values,\n",
        "        patch_size,\n",
        "        batch_size,\n",
        "        3),\n",
        "    steps_per_epoch=steps,\n",
        "    validation_data = SeismicSequence(\n",
        "        labeled_data,\n",
        "        test_data[\"Xline\"].values,\n",
        "        test_data[\"Time\"].values,\n",
        "        test_data[\"Class\"].values,\n",
        "        patch_size,\n",
        "        batch_size,\n",
        "        3),\n",
        "    validation_steps = len(test_data)//batch_size,\n",
        "    epochs = epochs,\n",
        "    verbose = 0,\n",
        "    callbacks = callbacklist)\n",
        "\n",
        "print('--> Training for ResNet transfer took {:.1f} sec'.format(time.time()-t0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmyBzboEuMiV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "resnet.save('resnet_model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "4ruw-jaZuMia",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "resnetscore=resnet.evaluate(np.array([patch_extractor2D(labeled_data,labels[\"Xline\"][x],labels[\"Time\"][x],patch_size,3) for x in test_samples]), keras.utils.to_categorical(labels[\"Class\"][test_samples], num_classes=9))\n",
        "print(acc_assess(resnetscore,lossfkt,metrica))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "MsYxl7dHuMic",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t_max, y_max = xline_data.shape\n",
        "\n",
        "half_patch = patch_size//2\n",
        "\n",
        "resnet_predx = np.full_like(xline_data,-1)\n",
        "\n",
        "for space in tqdm_notebook(range(y_max),desc='Space'):\n",
        "    for depth in tqdm_notebook(range(t_max),leave=False, desc='Time'):\n",
        "        resnet_predx[depth,space] = np.argmax(resnet.predict(np.expand_dims(patch_extractor2D(xline_data,space,depth,patch_size,3), axis=0)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZtYASVzuMie",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.save('resnet_predx.npy',resnet_predx,allow_pickle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNIFJkvJuMii",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.imshow(resnet_predx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQHQOVh0uMik",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig2 = plt.figure(figsize=(15.0, 10.0))\n",
        "img2 = plt.imshow(xline_data, cmap=\"Greys\", vmin=-vmx, vmax=vmx, aspect='auto')\n",
        "img1 = plt.imshow(resnet_predx, aspect='auto', cmap=\"Dark2\", alpha=0.8)\n",
        "plt.savefig('resnet_x.png', bbox_inches='tight')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "_FxlhalWuMim",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t_max, y_max = inline_data.shape\n",
        "\n",
        "half_patch = patch_size//2\n",
        "\n",
        "resnet_predi = np.full_like(inline_data,-1)\n",
        "\n",
        "for space in tqdm_notebook(range(y_max-400,y_max-300),desc='Space'):\n",
        "    for depth in tqdm_notebook(range(t_max-400,t_max-300),leave=False, desc='Time'):\n",
        "        resnet_predi[depth,space] = np.argmax(resnet.predict(np.expand_dims(patch_extractor2D(inline_data,space,depth,patch_size,3), axis=0)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPvLVrM-uMio",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.save('resnet_predi.npy',resnet_predi,allow_pickle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dE2fjKI1uMip",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.imshow(resnet_predi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wb51LEwAuMir",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig2 = plt.figure(figsize=(15.0, 10.0))\n",
        "img2 = plt.imshow(inline_data, cmap=\"Greys\", vmin=-vmy, vmax=vmy, aspect='auto')\n",
        "img1 = plt.imshow(resnet_predi, aspect='auto', cmap=\"Dark2\", alpha=0.8)\n",
        "plt.savefig('resnet_i.png', bbox_inches='tight')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vw8N6K6quMit",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(res_hist.history.keys())\n",
        "plt.plot(res_hist.history['acc'])\n",
        "plt.plot(res_hist.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfCmdLGNuMiu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# summarize history for loss\n",
        "plt.plot(res_hist.history['loss'])\n",
        "plt.plot(res_hist.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "1_QGRTjmuMiw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_model(resnet, to_file='model_resnet.png')\n",
        "plot_model(resnet, to_file='model_resnet_shapes.png', show_shapes=True)\n",
        "SVG(model_to_dot(resnet).create(prog='dot', format='svg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABZZ6W5DuMiy",
        "colab_type": "text"
      },
      "source": [
        "# Model Summary\n",
        "We can see the summaries of the layers in the model definitions. Leveraging high-dimensional CNNs that are already trained can be very valuable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVK7Oq41uMiy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_vanilla.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GprbpzobuMi0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vgg.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IldNoBuQuMi4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "resnet.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Soxb1c2iuMi5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}